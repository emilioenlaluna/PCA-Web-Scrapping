# -*- coding: utf-8 -*-
"""WebScrappingWorldHeritageInDanger.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f9kSyrKVdg9Be5O3yL8aq3J_tW9tU1D8

Importamos las librerias
"""

import pandas as pd
import numpy as np

"""Leemos el html y buscamos las tablas de la pagina usando pandas (se encontraron 5 tablas)"""

dfs = pd.read_html('https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger')
len(dfs)

"""El arreglo que contiene las tablas de la pagina, en la posicion 1 contine la tabla deseada"""

dfs[1].head()

"""Almacenamos en una variable la tabla para manipularla"""

data=dfs[1]

"""Eliminamos columnas que no aportan al PCA"""

data.drop('Image',inplace=True, axis=1)
data

data.drop('Refs',inplace=True, axis=1)
data

data.drop('Reason',inplace=True, axis=1)
data

data.drop('Name',inplace=True, axis=1)
data

"""Mostramos el dataframe que utilizaremos"""

data.head(10)

"""Mostramos los datos (Están sucios, necesitamos aplicar limpieza)"""

data['Area ha (acre)'].head(10)

"""Realizamos limpieza de los datos"""

# Limpiar y transformar datos
data['Criteria'] = np.where(data['Criteria'].str.contains('Natural'), 'Natural', 'Cultural')
data['Year (WHS)'] = pd.to_numeric(data['Year (WHS)'])
data['Endangered'] = data['Endangered'].str.extract('(\d{4})', expand=False).astype(float)

data['Area'] = data['Area ha (acre)'].str.extract(r'\((.*?)\)')

data['Area'] = pd.to_numeric(data['Area'].str.replace(',', ''))

data.drop('Location',inplace=True, axis=1)
data

data.drop('Area ha (acre)',inplace=True, axis=1)
data

data = data.fillna(0)

"""Mostramos datos limpios"""

data.head(10)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

# Assume 'col_name' is the name of the column you want to convert
data['Criteria'] = le.fit_transform(data['Criteria'])

data.head(10)

"""Impresión de las zonas en riesgo en mapa

Para obtener las ubicaciones realizaremos un nuevo scrapping directamente sobre la pagina
"""

import re
import numpy as np
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import folium

# Hacer la peticion HTTP Get para obtener la información
url = "http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

"""Encontramos la tabla, la convertimos a un dataframe, asignamos las columnas"""

# Encontramos la talba
table = soup.find_all("table", class_="wikitable")[0]

# Creamos el DataFrame con la tabla
df = pd.read_html(str(table), header=0)[0]

# Clean up the DataFrame
df = df.iloc[:, [0, 2, 3, 5, 6]]
df.columns = ["name", "location", "criteria", "year_inscribed", "year_endangered"]

"""Extraemos la latitud y longitud, creamos una funcion para convertir a grados decimales"""

# Extract latitude and longitude
df["latitude"] = df["location"].str.extract(r"(\d{1,2}°\d{1,2}′\d{1,2}″[NS])")
df["longitude"] = df["location"].str.extract(r"(\d{1,2}°\d{1,2}′\d{1,2}″[EW])")

# Function to convert DMS to decimal degrees
def dms2dd(dms):
    if pd.isna(dms):
        return np.nan

    match = re.match(r"(\d{1,2})°(\d{1,2})′(\d{1,2})″([NSWE])", dms)
    if not match:
        return np.nan

    degrees, minutes, seconds, direction = match.groups()
    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60)
    if direction in {'S', 'W'}:
        dd *= -1
    return dd

"""Aplicamos la funcion"""

# Convert DMS to decimal degrees
df["latitude"] = df["latitude"].astype(str).apply(dms2dd)
df["longitude"] = df["longitude"].astype(str).apply(dms2dd)
print(df)

"""Mostramos los datos en el mapa"""

# Se crea el mapa
mapa = folium.Map(location=[0, 0], zoom_start=2)

# Agrega los marcadores en el mapa 
for index, row in df.iterrows():
    if not pd.isna(row['latitude']) and not pd.isna(row['longitude']):
        folium.Marker(
            location=[row['latitude'], row['longitude']],
            popup=row['name'],
            icon=folium.Icon(color='orange',)
        ).add_to(mapa)

# Display the map
mapa

"""Analisis de Componentes principales

El Análisis de Componentes Principales (ACP) es una técnica de reducción de dimensionalidad que se utiliza para explorar y visualizar la estructura de los datos multivariados. El objetivo del ACP es resumir la información contenida en un conjunto de variables en un número menor de variables no correlacionadas llamadas componentes principales.

"""

pcaData=data

pcaData.head()

"""Instalamos factor analyzer"""

!pip install factor_analyzer

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from factor_analyzer import FactorAnalyzer
import numpy as np

# Data standardization
scaler = StandardScaler()
Datos_scaled = scaler.fit_transform(pcaData)

# Perform PCA
pca = PCA(n_components=4)
pca.fit(Datos_scaled)
res = pca.transform(Datos_scaled)

# Factor analysis to obtain factor loadings
fa = FactorAnalyzer(n_factors=5, rotation="varimax")
fa.fit(pcaData)

# Plotting
fig, ax = plt.subplots(figsize=(10,8))
ax.scatter(res[:, 0], res[:, 1], c="red")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
plt.title("PCA - Individual plot")

fig, ax = plt.subplots(figsize=(10,8))
ax.scatter(res[:, 0], res[:, 1], c="blue")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
plt.title("PCA - Variable plot")

fig, ax = plt.subplots(figsize=(10, 8))
corr_mat = fa.loadings_

# Create a circle
circle = plt.Circle((0,0), 1, color='black', fill=False)
ax.add_artist(circle)

# Add arrows
for i, (x, y) in enumerate(zip(corr_mat[:, 0], corr_mat[:, 1])):
    ax.arrow(0, 0, x, y, head_width=0.05, head_length=0.1, color='r')
    ax.text(x*1.1, y*1.1, pcaData.columns[i], color='r', ha='center', va='center')

# Set limits
plt.xlim(-1, 1)
plt.ylim(-1, 1)

plt.title("Circle of correlations")
plt.show()

#Results
print(pca.explained_variance_ratio_)
print(pca.components_)

"""Plano de similitud

El Plano de Similitud es una representación gráfica que muestra la similitud entre las observaciones en función de las variables que se han incluido en el análisis. En este plano, cada observación se representa como un punto y las distancias entre los puntos representan las diferencias o similitudes entre las observaciones.

Círculo de Correlaciones

El Círculo de Correlaciones es una representación gráfica que muestra la relación entre las variables originales y las componentes principales generadas por el ACP. En este círculo, cada variable se representa como un vector y la longitud y dirección del vector indica la fuerza y la dirección de la correlación entre la variable y las componentes principales.

Teorema de Dualidad

El Teorema de Dualidad establece que el Plano de Similitud y el Círculo de Correlaciones son dos representaciones equivalentes del mismo conjunto de datos. En otras palabras, la información contenida en el Plano de Similitud y en el Círculo de Correlaciones es la misma, pero se presenta de manera diferente. Esta dualidad es útil para interpretar y visualizar los resultados del ACP de diferentes maneras.
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA


#Load data
Datos = pd.read_csv("EjemploEstudiantes.csv", sep=";", decimal=",", index_col=0)

#Data standardization
scaler = StandardScaler()
Datos_scaled = scaler.fit_transform(Datos)

#Perform PCA
pca = PCA(n_components=5)
pca.fit(Datos_scaled)
res = pca.transform(Datos_scaled)

#Results
print(pca.explained_variance_ratio_)
print(pca.components_)

#Plotting
fig, ax = plt.subplots(figsize=(10,8))
ax.scatter(res[:, 0], res[:, 1], c="red")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
plt.title("PCA - Individual plot")

fig, ax = plt.subplots(figsize=(10,8))
ax.scatter(res[:, 0], res[:, 1], c="blue")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
plt.title("PCA - Variable plot")

#Circulo de correlaciones - Not sure what this is referring to.



import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from factor_analyzer import FactorAnalyzer
import numpy as np

# Load data
Datos = pd.read_csv("EjemploEstudiantes.csv", sep=";", decimal=",", index_col=0)

# Data standardization
scaler = StandardScaler()
Datos_scaled = scaler.fit_transform(Datos)

# Perform PCA
pca = PCA(n_components=5)
pca.fit(Datos_scaled)
res = pca.transform(Datos_scaled)

# Factor analysis to obtain factor loadings
fa = FactorAnalyzer(n_factors=5, rotation="varimax")
fa.fit(Datos)

# Plotting
fig, ax = plt.subplots(figsize=(10,8))
ax.scatter(res[:, 0], res[:, 1], c="red")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
plt.title("PCA - Individual plot")

fig, ax = plt.subplots(figsize=(10,8))
ax.scatter(res[:, 0], res[:, 1], c="blue")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
plt.title("PCA - Variable plot")

fig, ax = plt.subplots(figsize=(10, 8))
corr_mat = fa.loadings_

# Create a circle
circle = plt.Circle((0,0), 1, color='black', fill=False)
ax.add_artist(circle)

# Add arrows
for i, (x, y) in enumerate(zip(corr_mat[:, 0], corr_mat[:, 1])):
    ax.arrow(0, 0, x, y, head_width=0.05, head_length=0.1, color='r')
    ax.text(x*1.1, y*1.1, Datos.columns[i], color='r', ha='center', va='center')

# Set limits
plt.xlim(-1, 1)
plt.ylim(-1, 1)

plt.title("Circle of correlations")
plt.show()

"""Conclusiones

Bibliografia

Anexo (Otra manera de realizar el web scrapping)
"""

import pandas as pd # library for data analysis
import requests # library to handle requests
from bs4 import BeautifulSoup # library to parse HTML documents

# get the response in the form of html
wikiurl="https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger"
table_class="wikitable plainrowheaders sortable jquery-tablesorter"
response=requests.get(wikiurl)
print(response.status_code)

# parse data from the html into a beautifulsoup object
soup = BeautifulSoup(response.text, 'html.parser')
indiatable=soup.find('table',{'class':"wikitable"})

df=pd.read_html(str(indiatable))
# convert list to dataframe
df=pd.DataFrame(df[0])
print(df.head())